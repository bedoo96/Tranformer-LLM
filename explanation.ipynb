{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-04 10:23:33--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘data/shakespeare.txt’\n",
      "\n",
      "data/shakespeare.tx 100%[===================>]   1.06M  1.97MB/s    in 0.5s    \n",
      "\n",
      "2024-06-04 10:23:34 (1.97 MB/s) - ‘data/shakespeare.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "!wget -O data/shakespeare.txt  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary of the model that it can see or emit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text))) # vocabulary of our transformer (see or emit)\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the raw text, which is a string, into a sequence of intergers according to some vocabulary of possible elements. To do this, we create a mapping function (an encoder) from characters to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { char:idx for idx, char in enumerate(chars) } # dictionnary of character as key and index as value \n",
    "itos = { idx:char for idx, char in enumerate(chars) } # dictionnary of character as value and index as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encode a list s of character (e.g a word) to a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode a list of integers into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encode('Best Transformer ever'))\n",
    "print(decode(encode('Best Transformer ever')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, you can use what encoder you want (not specialy for each character). ChatGPT use [TikToken](https://github.com/openai/tiktoken) as encoder/decoder which is a sub-words units tokenizer. A tokenizer can be more complex than just a usuel cut, check [this video](https://www.youtube.com/watch?v=zduSFxRajkE) for more infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 90% will be train, rest val\n",
    "train = data[:int(0.9*len(data)) ]\n",
    "val = data[int(0.9*len(data)) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # number of parallel sequences will process (i.e number of line analyse in the same time)\n",
    "block_size = 8 # the maximum context length for predictions / \"time\" (i.e maximum number of character in this line which will be analyse)\n",
    "\n",
    "def get_batch(set, batch_size=batch_size, block_size=block_size):\n",
    "    data = train if set=='train' else val \n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,)) # random offsets into the training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return x.to(device), y.to(device) # input, target\n",
    "\n",
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None): # None because we use self() in the generate()\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) batch, time, channel(dimension of your embedding)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # (batch_size=4, block_size=8, vocab_size=65)\n",
    "            logits = logits.view(B*T, C) # need to reshape because of cross_entropy torch function implementation\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # negative log likelihood \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # for the moment idx are a integer if the input is torch.zeros((1, 1), dtype=torch.long); after it is a matrix of batch_size*time size\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the vector in the embedding space\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step token embedding\n",
    "            # (Here we feed all the character block but we just check the value of the last to generate the one after.)\n",
    "            # (Not really smart but will make sense with self attention.)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities (see it like an activation function to have an equal repartition between 0 and 1)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution to get an index number\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) because num_samples equal 1\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            #now index is a tensor of integer when the input was torch.zeros((1, 1), dtype=torch.long)\n",
    "        return idx\n",
    "\n",
    "m = ShakespeareLanguageModel(vocab_size).to(device)\n",
    "logits, loss = m(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist())) # zeros((1,1)) for generate from the first charater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's delve into the embedding layer**\n",
    "\n",
    "Embeddings serve as a method to represent data, such as tokens, in a high-dimensional continuous space. In this case, the space is represented by $\\mathbb{R}^{\\text{vocab size}}$, cause the second parameter of `nn.Embedding` is the vocabulary size. The input has to be one-hot-encode and that is why we need to precise the vocabulary size in the first parameter. Training this layer involves shifting each vector within this space.\n",
    "\n",
    "One of the simplest ways to visualize this concept is by attempting to determine whether certain words are positive or negative, and whether they are commonly used or formal. Imagine projecting your words (or tokens) onto a two-dimensional plane, where each hyperplane from the canonic base represents a particular state. For instance, if a vector falls within $\\mathbb{R}^{+,+}$, it signifies that the word is both positive and formal.\n",
    "\n",
    "In the task of predicting the next word, you can utilize the block of encode words as the input. By adding all the emdedding vectors together, you can then decode the nearest emdedding token to this resultant addition in this space, yielding the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ">>> # an Embedding module containing 10 tensors of dimension 3\n",
    ">>> embedding = nn.Embedding(10, 3)\n",
    ">>> # a batch of 2 samples of 4 token of each\n",
    ">>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    ">>> embedding(input)\n",
    "tensor([[[-0.0251, -1.6902,  0.7172],\n",
    "         [-0.6431,  0.0748,  0.6969],\n",
    "         [ 1.4970,  1.3448, -0.9685],\n",
    "         [-0.3677, -2.7265, -0.1685]],\n",
    "\n",
    "        [[ 1.4970,  1.3448, -0.9685],\n",
    "         [ 0.4362, -0.4004,  0.9400],\n",
    "         [-0.6431,  0.0748,  0.6969],\n",
    "         [ 0.9124, -2.3616,  1.1151]]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "epochs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #context manager \n",
    "def estimate_loss(model=m, epochs=epochs):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(epochs)\n",
    "        for k in range(epochs):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    \n",
    "    if _ % 500 == 0 or iter == epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {_}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "    x, y = get_batch('train')\n",
    "    \n",
    "    logits, loss = m(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist())) # zeros((1,1)) for generate from the first charater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! But for the moment, each token is only created thanks to the previous one instead of all the previous ones : need to add attention in it !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "z = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st step : get the average of the precedent tokens (\"bag of words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "zbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        zprev = z[b,:t+1]\n",
    "        zbow[b, t] = torch.mean(zprev, 0)\n",
    "print(z[0])\n",
    "print(zbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder that zbow is in the embedding space, so change in my exemple (in the paragraph explanation of embedding layer above) the sum into the mean per component to understand what is the output.\n",
    "\n",
    "Let's optimize this code with a mathematical trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "print(a) # so a@b is the average of the precedent time of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "zbow2 = wei @ z # here torch will convert this (T, T)@(B, T, C) product to a (B, T, T)@(B, T, C) to match the dimension --> (B, T, C)\n",
    "torch.allclose(zbow, zbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But with averaging, we lose a lot of information, such as the position or importance of a token in relation to the one to be predicted. Let's try adapting this weight matrix to modify the weights of previous tokens to predict the next one. Let's add some self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2th step : self-attention !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's delve into self-attention every single** \n",
    "\n",
    "Each token will emit independently two new vectors : \n",
    "- `query` : \"what I am looking for\"\n",
    "- `key` : \"what do I contain\"\n",
    "\n",
    "So, to ensure that one token's query is correctly \"aligned\" with another token's key, we need to check whether these two vectors are LITERALLY aligned. This is why dot product have been created. So now the weights of the matrice is representing by the dot product between the query of the token to predict and the key of all the precedent ones.\n",
    "\n",
    "Note that the `query` and the `key` vectors are created from the emdedding vector of the token and not directly from the token.\n",
    "\n",
    "Let's see a single Head perform self-attention !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16 # the length of the vector of the query/key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = nn.Linear(C, head_size, bias=False) # remove the biais because I DONT KNOW WHY YET\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(z) # (B, T, 16)\n",
    "q = query(z) # (B, T, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product <x,y> can be write as x @ y.T for row vectors\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -->  (B, T, T)\n",
    "print(wei.shape)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool but it seems that the token can interact with the following ones. Let's mask the next ones and re distribute that !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(tril)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei[0])\n",
    "wei = F.softmax(wei, dim=-1) # nice distribution equal to one\n",
    "print(wei[0])\n",
    "\n",
    "out = wei @ z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool but when input Q,K are not unit variance, wei will be an explozing variance and Softmax will not stay diffuse but it will saturate too much (creating an one-hot vector, that means that the target token will get information from one unique other vector). We need to force Q, K to be unit variance by normalize with $\\sqrt{d_k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = q @ k.transpose(-2, -1)* head_size**-0.5 # (B, T, 16) @ (B, 16, T) -->  (B, T, T)\n",
    "print(wei.shape)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # nice distribution equal to one\n",
    "\n",
    "out = wei @ z\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, each token will emit one more vector :\n",
    "- `values` : \"what I will communicate to the token if it find me interesting\"\n",
    "\n",
    "And the output of all of this will be the matrix product between the weights and the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = nn.Linear(C, head_size, bias=False)\n",
    "v = value(z)\n",
    "\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understand why the value layer is necessary with an example**\n",
    "\n",
    "Imagine the sequence “My black cat died yesterday. In his coffin, he looked TARGET”. Here, we're looking for an adjective to describe the cat. But in theory, the weights of “black” and “dead” should be very close, as they're both adjectives describing the cat. The result could therefore be either an adjective close to black, or an adjective close to dead. That's why we add the value layer: here, death brings much more value than black. What we mean is that even though we're looking for an adjective, we want an adjective to correlate with death, so “death” has to have a higher value than “black”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION OF SELF-ATTENTION**\n",
    "\n",
    "To really understand what's going on here\n",
    "1. In the time block, at each instant t, i.e. at each new token to be predicted, you take its key vector and search for the most aligned (literraly) previous vectors in the block using the dot product with their query vectors. The weight matrix is now the matrix of each dot product.\n",
    "2. Now in the embedding space, wei @ z represent the shifting to the **weighted** average of the precedent tokens for the token to predict.\n",
    "3. But we want more freedom to really understand what matters (and not just what aligns) in a sentence. So we add a new layer name value which represent the value of each token in the sequence. And now the output become wei @ value(z)\n",
    "\n",
    "So to have a high vector output (so to bring the original embedding token vector to), you have to be interesting for the prediction (represente by the weight) AND add a hugh value to the sequence (representing by the value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, **allowing all tokens to communicate (past and future)**. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #is not a paremeter\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1)* C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v=self.value(x)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model (part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change the size of the embedding space ! \n",
    "2. And get information from position of the tokens ! \n",
    "3. 1.+2.\n",
    "4. Add parralel attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 32\n",
    "head_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakePT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        #### 1. change the dimension of the token embedding ####\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, dim_emb)\n",
    "        #### 2. create embedding for the position of the tokens ####\n",
    "        self.position_embedding_table = nn.Embedding(block_size, dim_emb)\n",
    "        #### 4. Attention ####\n",
    "        self.head = OneHead(dim_emb, head_size=dim_emb) #head_size=dim_emb for the moment cause we do not introduce mutliheading yet (to match the dimensions)\n",
    "        self.linear = nn.Linear(dim_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        #### 1. #####\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        #new : logits = self.linear(tok_emb) # (B, T, vocab_size); is needed to match the output for cross_entropy function\n",
    "        #### 2. #####\n",
    "        B, T = idx.shape\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        #### 3. ####\n",
    "        x = tok_emb + pos_emb\n",
    "        #### 4. ####\n",
    "        x = self.head(x)\n",
    "        \n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:, -block_size:] #make sure that the idx that are feed into the model has no more than block size coming in (position_embedding_table)\n",
    "            logits, loss = self(idx_crop)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = ShakePT(vocab_size, dim_emb, head_size).to(device)\n",
    "logits, loss = m(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do it run mutliple attention in parralel and concatenating the results instead of one attention with a huge head size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([OneHead(dim_emb, head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1) #contatenation over the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakePT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, dim_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, dim_emb)\n",
    "        self.head = MultipleHeadAttention(4, dim_emb, dim_emb//4) # instead of having one big attention head, we divise it in 4 then concatenate\n",
    "        self.linear = nn.Linear(dim_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.head(x)\n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_crop)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = ShakePT(vocab_size, dim_emb, head_size).to(device)\n",
    "logits, loss = m(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new layer of the model because it can not really deside by is own which prediction to do, i.e it just decode the high value embedding vector output as a target. This new layer enable to decide if this vector is a good choice or no. This layer is just a MLP with a ReLu activation function (OpenAI use GeLu). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim_emb, dim_emb),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakePT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, dim_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, dim_emb)\n",
    "        self.head = MultipleHeadAttention(4, dim_emb, dim_emb//4)\n",
    "        self.fforward = FeedForward(dim_emb) # neeeeew\n",
    "        self.linear = nn.Linear(dim_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.head(x)\n",
    "        x = self.fforward(x) # neeeeew\n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_crop)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = ShakePT(vocab_size, dim_emb, head_size).to(device)\n",
    "logits, loss = m(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's group and scale up this model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to go deeper by replicate this model. But the problem of vanishing gradient will apear, so we use three optimization tools : \n",
    "- skip connection / residual connection (like ResNet)\n",
    "- add a layer norm (BEFORE as pre-norm formulation, not as the original paper)\n",
    "- add dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        k = self.key(x) \n",
    "        q = self.query(x) \n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1)* C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei) #add dropout\n",
    "        \n",
    "        v=self.value(x)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([OneHead(dim_emb, head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(dim_emb, dim_emb) # add a projection layer \n",
    "        self.dropout = nn.Dropout(0.1) # add a dropout layer \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim_emb, 4*dim_emb), # the paper multiply by 4 the inner-layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*dim_emb, dim_emb), # add a linear layer\n",
    "            nn.Dropout(0.1), # add dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim_emb):\n",
    "        super().__init__()\n",
    "        self.heads = MultipleHeadAttention(4, dim_emb, dim_emb//4)\n",
    "        self.fforward = FeedForward(dim_emb)\n",
    "        self.ln1 = nn.LayerNorm(dim_emb)\n",
    "        self.ln2 = nn.LayerNorm(dim_emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = self.ln1(x) # layer norm\n",
    "        x += self.heads(x_) # with skip connection\n",
    "        x_ = self.ln2(x) # layer norm\n",
    "        x += self.fforward(x) # with skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakePT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_emb):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, dim_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, dim_emb)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(dim_emb),\n",
    "            Block(dim_emb),\n",
    "            Block(dim_emb),\n",
    "            nn.LayerNorm(dim_emb)\n",
    "        )\n",
    "        self.linear = nn.Linear(dim_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x) # use the blocks\n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_crop)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = ShakePT(vocab_size, dim_emb).to(device)\n",
    "logits, loss = m(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it ! You create your state of the art transformer. You can now see a \"clean\" code on the *ShakePT.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOME NOTES TO READ :**\n",
    "- we create a decoder transformer instead of an complete (encoder+decoder) transformer. Because we just want to generate one language speetch. If you want to translate, you have to \"encode\" your initial speek language setence into the embedding space, and its keys and values are send to the multi-head attention (like cross attention). Becarefull because during the \"encode\", the prediction have access to all the token block.\n",
    "- much more optimization can be bring (as parralelize the multiple head attention instead of continating)\n",
    "- we have created an general pre-trained transformer, not an assistant. So if you ask a question, it could reply with other questions link. You have to specialize this transfomers into an assistant with **fine-tuning**.\n",
    "- OpenIA use also a reward model after the fine tuning to push ChatGPT to reply with high reward answers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
