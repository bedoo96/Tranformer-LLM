{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-19 14:49:31--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘data/shakespeare.txt’\n",
      "\n",
      "data/shakespeare.tx 100%[===================>]   1.06M  3.38MB/s    in 0.3s    \n",
      "\n",
      "2025-03-19 14:49:32 (3.38 MB/s) - ‘data/shakespeare.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "!wget -O data/shakespeare.txt  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(' '.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Convert the raw text, which is a string into sequence of integers according to some vocabulary of possible elelments.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char:idx for idx, char in enumerate(chars)} # dictionnary of character as key and index as value \n",
    "itos = {idx:char for idx, char in enumerate(chars)} # dictionnary of character as value and index as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 43, 57, 58, 1, 32, 56, 39, 52, 57, 44, 53, 56, 51, 43, 56, 1, 43, 60, 43, 56]\n",
      "Best Transformer ever\n"
     ]
    }
   ],
   "source": [
    "print(encode('Best Transformer ever'))\n",
    "print(decode(encode('Best Transformer ever')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train and Validation Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 90% will be train, rest for val\n",
    "\n",
    "train_ds = data[:int(0.9*len(data))]\n",
    "val_ds = data [int(0.9*len(data)) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numbe of sequence that will be processed\n",
    "batch_size = 16\n",
    "# The maximum context length for prediction\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(set, batch_size = batch_size , block_size =block_size):\n",
    "    data = train_ds if set == 'train_ds' else val_ds\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in idx])\n",
    "    y =  torch.stack([data[i + 1 : i + block_size + 1] for i in idx])\n",
    "\n",
    "    return x.to(device), y.to(device) # input and target\n",
    "\n",
    "x, y = get_batch(\"train_ds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create the initial model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 65])\n",
      "tensor(4.8517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "dTu&!HjOpI!dmi. yD&ZwIuwjWmUvegtRWcrqC&bSZltK:.UIjxQQICa;us.hVkXU\n",
      "DaP:,'ZOx3wXHNZwCxzqTy?gVtFXaWUA&G\n"
     ]
    }
   ],
   "source": [
    "class ShakespeareLangueModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C), batch, time, channel)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T, C = logits.shape #(batch_size =16, Block_size = 8, vocab_size =65)\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the vector in the embedding space\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on the last time step token embedding\n",
    "            # (Here we feed all the character block but we just check the value of the last to generate the one after.)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities \n",
    "            probs =F.softmax(logits, dim = 1)\n",
    "            # sample from distribution to get an index number\n",
    "            idx_next = torch.multinomial(probs, num_samples= 1) # (B, 1) because num_samples equal 1\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim =1)\n",
    "            # now index is a tensor of integer when the input was\n",
    "            # torch.zeros((1,1), dtypes = torch.long)\n",
    "        return idx\n",
    "\n",
    "m = ShakespeareLangueModel(vocab_size).to(device)\n",
    "logits, loss = m(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist())) # zeros((1,1)) for generate from the first charater       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's delve into the embedding layer**\n",
    "\n",
    "Embeddings serve as a method to represent data, such as tokens, in a high-dimensional continuous space. In this case, the space is represented by $\\mathbb{R}^{\\text{vocab size}}$, cause the second parameter of `nn.Embedding` is the vocabulary size. The input has to be one-hot-encode and that is why we need to precise the vocabulary size in the first parameter. Training this layer involves shifting each vector within this space.\n",
    "\n",
    "One of the simplest ways to visualize this concept is by attempting to determine whether certain words are positive or negative, and whether they are commonly used or formal. Imagine projecting your words (or tokens) onto a two-dimensional plane, where each hyperplane from the canonic base represents a particular state. For instance, if a vector falls within $\\mathbb{R}^{+,+}$, it signifies that the word is both positive and formal.\n",
    "\n",
    "In the task of predicting the next word, you can utilize the block of encode words as the input. By adding all the emdedding vectors together, you can then decode the nearest emdedding token to this resultant addition in this space, yielding the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #context manager \n",
    "def estimate_loss(model=m, epochs=epochs):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(epochs)\n",
    "        for k in range(epochs):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.8615, val loss 2.8628\n",
      "step 500: train loss 2.7451, val loss 2.7470\n",
      "step 1000: train loss 2.6735, val loss 2.6734\n",
      "step 1500: train loss 2.6241, val loss 2.6265\n",
      "step 2000: train loss 2.5890, val loss 2.5900\n",
      "step 2500: train loss 2.5644, val loss 2.5669\n",
      "step 3000: train loss 2.5462, val loss 2.5448\n",
      "step 3500: train loss 2.5326, val loss 2.5315\n",
      "step 4000: train loss 2.5215, val loss 2.5216\n",
      "step 4500: train loss 2.5106, val loss 2.5139\n",
      "step 5000: train loss 2.5086, val loss 2.5057\n",
      "step 5500: train loss 2.5025, val loss 2.5015\n",
      "step 6000: train loss 2.4978, val loss 2.4994\n",
      "step 6500: train loss 2.4934, val loss 2.4955\n",
      "step 7000: train loss 2.4941, val loss 2.4926\n",
      "step 7500: train loss 2.4897, val loss 2.4902\n",
      "step 8000: train loss 2.4871, val loss 2.4876\n",
      "step 8500: train loss 2.4860, val loss 2.4867\n",
      "step 9000: train loss 2.4863, val loss 2.4838\n",
      "step 9500: train loss 2.4829, val loss 2.4843\n",
      "\n",
      "hoouearele d the. tilouly utemeds IVO:\n",
      "In hinot lt ves the r gs t f.\n",
      "Wascke, SCu tht walyor's:\n",
      "JUA C\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    if _ % 500 == 0 or iter ==epochs -1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {_}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    x , y = get_batch('train_ds')\n",
    "\n",
    "    logits , loss = m(x ,y)\n",
    "    optimizer.zero_grad(set_to_none= True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist())) # zeros((1,1)) for generate from the first charater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD Attention is all you need !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "B , T, C = 4, 8, 2\n",
    "z =torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Get the Average of the precedent tokens (\"bag of words)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5295,  1.8478],\n",
      "        [-0.8244,  1.6689],\n",
      "        [-1.0310, -1.1666],\n",
      "        [ 0.5904, -0.9030],\n",
      "        [ 0.0213, -1.4140],\n",
      "        [-0.0675, -1.2013],\n",
      "        [ 1.2422, -0.9646],\n",
      "        [ 0.3955,  0.6795]])\n",
      "tensor([[ 0.5295,  1.8478],\n",
      "        [-0.1474,  1.7584],\n",
      "        [-0.4420,  0.7834],\n",
      "        [-0.1839,  0.3618],\n",
      "        [-0.1428,  0.0066],\n",
      "        [-0.1303, -0.1947],\n",
      "        [ 0.0658, -0.3047],\n",
      "        [ 0.1070, -0.1817]])\n"
     ]
    }
   ],
   "source": [
    "zbow = torch.zeros((B, T, C))\n",
    "for b in range (B):\n",
    "    for t in range (T):\n",
    "        zprev = z [b, : t+1]\n",
    "        zbow[b, t] = torch.mean(zprev, 0)\n",
    "print(z[0])\n",
    "print(zbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's optmize the code with a mathematical trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "print(a) # so a@b is the average of the precedent time of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "zbow2 = wei @ z # here torch will convert this (T, T)@(B, T, C) product to a (B, T, T)@(B, T, C) to match the dimension --> (B, T, C)\n",
    "torch.allclose(zbow, zbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's delve into self-attention every single** \n",
    "\n",
    "Each token will emit independently two new vectors : \n",
    "- `query` : \"what I am looking for\"\n",
    "- `key` : \"what do I contain\"\n",
    "\n",
    "So, to ensure that one token's query is correctly \"aligned\" with another token's key, we need to check whether these two vectors are LITERALLY aligned. This is why dot product have been created. So now the weights of the matrice is representing by the dot product between the query of the token to predict and the key of all the precedent ones.\n",
    "\n",
    "Note that the `query` and the `key` vectors are created from the emdedding vector of the token and not directly from the token.\n",
    "\n",
    "Let's see a single Head perform self-attention !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16 # The length of the input query vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(z) # (B,T, 16)\n",
    "q = query(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3986, -0.4121,  1.3382,  0.1287,  0.7916,  0.7284, -0.2621, -0.6455],\n",
       "        [ 1.5054,  2.3810, -0.4043, -1.4015, -1.4864, -1.1957, -1.9765,  0.3962],\n",
       "        [ 2.3637,  1.7327, -1.7075, -0.8930, -1.6771, -1.4512, -0.7649,  0.9313],\n",
       "        [-1.1212, -1.5933,  0.3974,  0.9264,  1.0481,  0.8523,  1.2622, -0.3229],\n",
       "        [ 0.1637, -0.5862, -0.4959,  0.3985,  0.1151,  0.0495,  0.7703,  0.1733],\n",
       "        [ 0.3210, -0.3170, -0.5273,  0.2388, -0.0469, -0.0797,  0.5400,  0.2116],\n",
       "        [-2.4976, -2.9946,  1.1818,  1.7022,  2.1532,  1.7807,  2.1650, -0.8046],\n",
       "        [-0.9411, -0.5759,  0.7407,  0.2813,  0.6304,  0.5536,  0.1718, -0.3883]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product <x,y> can be write as x @ y.T for row vectors\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -->  (B, T, T)\n",
    "print(wei.shape)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's mask the next ones and re distribute that !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[-1.3986,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 1.5054,  2.3810,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 2.3637,  1.7327, -1.7075,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.1212, -1.5933,  0.3974,  0.9264,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1637, -0.5862, -0.4959,  0.3985,  0.1151,    -inf,    -inf,    -inf],\n",
      "        [ 0.3210, -0.3170, -0.5273,  0.2388, -0.0469, -0.0797,    -inf,    -inf],\n",
      "        [-2.4976, -2.9946,  1.1818,  1.7022,  2.1532,  1.7807,  2.1650,    -inf],\n",
      "        [-0.9411, -0.5759,  0.7407,  0.2813,  0.6304,  0.5536,  0.1718, -0.3883]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2941, 0.7059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6455, 0.3435, 0.0110, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0717, 0.0447, 0.3276, 0.5560, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2377, 0.1123, 0.1229, 0.3006, 0.2264, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2359, 0.1246, 0.1010, 0.2173, 0.1633, 0.1580, 0.0000, 0.0000],\n",
      "        [0.0026, 0.0016, 0.1014, 0.1707, 0.2680, 0.1846, 0.2712, 0.0000],\n",
      "        [0.0396, 0.0570, 0.2128, 0.1344, 0.1905, 0.1765, 0.1204, 0.0688]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(tril)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei[0])\n",
    "wei = F.softmax(wei, dim=-1) # nice distribution equal to one\n",
    "print(wei[0])\n",
    "\n",
    "out = wei @ z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool but when input Q,K are not unit variance, wei will be an explozing variance and Softmax will not stay diffuse but it will saturate too much (creating an one-hot vector, that means that the target token will get information from one unique other vector). We need to force Q, K to be unit variance by normalize with $\\sqrt{d_k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = q @ k.transpose(-2, -1)* head_size**-0.5 # (B, T, 16) @ (B, 16, T) -->  (B, T, T)\n",
    "print(wei.shape)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # nice distribution equal to one\n",
    "\n",
    "out = wei @ z\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, each token will emit one more vector :\n",
    "- `values` : \"what I will communicate to the token if it find me interesting\"\n",
    "\n",
    "And the output of all of this will be the matrix product between the weights and the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size, bias=False)\n",
    "v = value(z)\n",
    "\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understand why the value layer is necessary with an example**\n",
    "\n",
    "Imagine the sequence “My black cat died yesterday. In his coffin, he looked TARGET”. Here, we're looking for an adjective to describe the cat. But in theory, the weights of “black” and “dead” should be very close, as they're both adjectives describing the cat. The result could therefore be either an adjective close to black, or an adjective close to dead. That's why we add the value layer: here, death brings much more value than black. What we mean is that even though we're looking for an adjective, we want an adjective to correlate with death, so “death” has to have a higher value than “black”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION OF SELF-ATTENTION**\n",
    "\n",
    "To really understand what's going on here\n",
    "1. In the time block, at each instant t, i.e. at each new token to be predicted, you take its key vector and search for the most aligned (literraly) previous vectors in the block using the dot product with their query vectors. The weight matrix is now the matrix of each dot product.\n",
    "2. Now in the embedding space, wei @ z represent the shifting to the **weighted** average of the precedent tokens for the token to predict.\n",
    "3. But we want more freedom to really understand what matters (and not just what aligns) in a sentence. So we add a new layer name value which represent the value of each token in the sequence. And now the output become wei @ value(z)\n",
    "\n",
    "So to have a high vector output (so to bring the original embedding token vector to), you have to be interesting for the prediction (represente by the weight) AND add a hugh value to the sequence (representing by the value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHead(nn.Module):\n",
    "    def __init__(self, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(dim_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #is not a paremeter\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1)* C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v=self.value(x)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model (part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change the size of the embedding space ! \n",
    "2. And get information from position of the tokens ! \n",
    "3. 1.+2.\n",
    "4. Add parralel attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_emb = 32 \n",
    "head_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 65])\n",
      "tensor(4.1968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "oeLJ;d;&gTW!Ddqqo!Wz.gh..bLnMSWT\n",
      "jZNStPbykm?P-JtebYJVonI\n",
      ";cvqs''Uhc;UoEM\n",
      "WN-tNkVGY;: BRvlwFa;DqUZS dHHrQra!PQ:?pZrDtZxEO\n",
      "fJv$pqEYAeSoxJmYaUvygLGtaXTSTs'OMrDB\n",
      "z-a,cKcr-wKS.NlKo'cqyDclVVX!PaAeZMTc-m;$L,;QyKwKOlXeCNheDSg?OZyULy&fkiySzAmR;zOTq$cz'OcJ?kv?\n",
      "I'VeSx-'sgWyWPXrGGnEeEELDkcemNZZ-wVF!FazUSp3C'yVKI?mExmd?YJT$.icxlYJ3vhosY-jYcWl?p?'b&,C;n;WtiQ;LfW.!iiWBiZ,xai?K;dPH.DlU\n",
      "enZecQMO.Ofv!se&.ZLI!Bq'Nc;wG&dSH-;KaamPHxEQKG\n",
      "oUEezHLj!tf-eszfoGKOPT-T&\n",
      ".Vs'''3n,'kUUrBdoT$Hz3nmyAcNM!\n",
      "Sq.bNahTcrZQESMA!nGxOHc\n"
     ]
    }
   ],
   "source": [
    "class ShakePT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_emb, head_size):\n",
    "        super().__init__()\n",
    "        ## 1. change the dimension of the token embedding ####\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, dim_emb)\n",
    "        ## 2. create embedding for the position of the tokens ####\n",
    "        self.position_embedding_table = nn.Embedding(block_size, dim_emb)\n",
    "        ##  Add one Attention ##\n",
    "        self.head = OneHead(dim_emb, head_size=dim_emb) #head_size=dim_emb for the moment cause we do not introduce mutliheading yet (to match the dimensions)\n",
    "        self.linear = nn.Linear(dim_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)        \n",
    "        B, T = idx.shape\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.head(x)\n",
    "        \n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:, -block_size:] #make sure that the idx that are feed into the model has no more than block size coming in (position_embedding_table)\n",
    "            logits, loss = self(idx_crop)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = ShakePT(vocab_size, dim_emb, head_size).to(device)\n",
    "logits, loss = m(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
